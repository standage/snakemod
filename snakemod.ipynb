{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snakemake\n",
    "from subprocess import run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity in Snakemake workflows\n",
    "\n",
    "The Snakemake workflow engine has proven itself useful in creating, maintaining, and extending a variety of our bioinformatics analysis workflows.\n",
    "We use Snakemake to create both single-purpose workflows as well as workflows that we expect to use and re-use frequently over an extended period of time.\n",
    "Accordingly, we will be faced with two recurrent questions.\n",
    "\n",
    "1. Which parts of which workflows can be re-used?\n",
    "2. What is the best strategy for modularity and re-use?\n",
    "\n",
    "We don't yet have a confident answer for those questions in the context of NBFAC workflows.\n",
    "The purpose of this workshop is to describe three complementary strategies to creating workflows or workflow components that can be maintained in a single place and re-used in numerous contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synopsis\n",
    "\n",
    "1. **Includes** are typically a small set of re-usable rules that get embedded into a larger workflow and share its configuration. This is the simplest way to re-use workflow components.\n",
    "2. **Subworkflows** allow one to invoke a distinct and self-contained workflow from another workflow. Each workflow can has its own configuration, working directory, and so on.\n",
    "3. **Wrappers** are used at the rule level, and provide the smallest building block for constructing workflows from re-usable parts. Snakemake maintains a public repository of wrappers for common tools, but Snakemake can also make use of private or local wrapper definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example workflow\n",
    "\n",
    "![Workflow](workflow.png)\n",
    "\n",
    "The demo workflow for this workshop is a fairly simple and linear workflow. In summary, it:\n",
    "\n",
    "- Downsamples Illumina reads to a user-specified number of read pairs using seqtk\n",
    "- Assembles the downsampled reads using SPAdes\n",
    "- Maps the original reads back to the assembled contigs using Bowtie2\n",
    "- Converts the alignment output in SAM format to BAM format, and sorts the aligned reads by position using SAMtools\n",
    "- Calculates some summary statistics from the read mappings using SAMtools\n",
    "\n",
    "The workflow consists of 9 rules: 3 related to proprocessing and assembly, 5 related to read mapping, and the 1 \"default\" rule to rule them all.\n",
    "With subworkflows and includes, we can explore how to re-use groups of related rules.\n",
    "With wrappers, we can explore how to replace rules running commonly used software with standardized invocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Before we get started, let's download some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 35.4M  100 35.4M    0     0   207k      0  0:02:54  0:02:54 --:--:--  244k0:02:32  0:00:01  0:02:31  238k30  335k 0:02:56  0:02:45  0:00:11  275k\n",
      "Read 129850 spots for SRR5944233.1.sra\n",
      "Written 129850 spots for SRR5944233.1.sra\n"
     ]
    }
   ],
   "source": [
    "!./getdata.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a reference for our exercises, let's run the example Snakemake workflow using the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating specified working directory sandbox/WD1/.\n",
      "Building DAG of jobs...\n",
      "Using shell: /bin/bash\n",
      "Provided cores: 4\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tall\n",
      "\t1\tbam_stats\n",
      "\t1\tcopyseq\n",
      "\t1\tdownsample\n",
      "\t1\tindex_assembly\n",
      "\t1\tindex_bam\n",
      "\t1\tmap_back_reads\n",
      "\t1\tmap_sort\n",
      "\t1\tspades\n",
      "\t9\n",
      "\n",
      "[Thu Oct 15 15:36:01 2020]\n",
      "rule copyseq:\n",
      "    input: /Users/standage/Projects/snakemod/sample057-R1.fastq, /Users/standage/Projects/snakemod/sample057-R2.fastq\n",
      "    output: seq/reads-R1.fastq, seq/reads-R2.fastq\n",
      "    jobid: 6\n",
      "\n",
      "[Thu Oct 15 15:36:02 2020]\n",
      "Finished job 6.\n",
      "1 of 9 steps (11%) done\n",
      "\n",
      "[Thu Oct 15 15:36:02 2020]\n",
      "rule downsample:\n",
      "    input: seq/reads-R1.fastq, seq/reads-R2.fastq\n",
      "    output: seq/reads-subset-R1.fastq, seq/reads-subset-R2.fastq\n",
      "    jobid: 8\n",
      "\n",
      "[Thu Oct 15 15:36:02 2020]\n",
      "Finished job 8.\n",
      "2 of 9 steps (22%) done\n",
      "\n",
      "[Thu Oct 15 15:36:02 2020]\n",
      "rule spades:\n",
      "    input: seq/reads-subset-R1.fastq, seq/reads-subset-R2.fastq\n",
      "    output: analysis/spades/scaffolds.fasta\n",
      "    jobid: 5\n",
      "    threads: 4\n",
      "\n",
      "[Thu Oct 15 15:46:15 2020]\n",
      "Finished job 5.\n",
      "3 of 9 steps (33%) done\n",
      "\n",
      "[Thu Oct 15 15:46:15 2020]\n",
      "rule index_assembly:\n",
      "    input: analysis/spades/scaffolds.fasta\n",
      "    output: analysis/spades/scaffolds.fasta.1.bt2, analysis/spades/scaffolds.fasta.2.bt2, analysis/spades/scaffolds.fasta.3.bt2, analysis/spades/scaffolds.fasta.4.bt2, analysis/spades/scaffolds.fasta.rev.1.bt2, analysis/spades/scaffolds.fasta.rev.2.bt2\n",
      "    jobid: 7\n",
      "\n",
      "[Thu Oct 15 15:46:19 2020]\n",
      "Finished job 7.\n",
      "4 of 9 steps (44%) done\n",
      "[Thu Oct 15 15:46:19 2020]\n",
      "\n",
      "group job 3fa3d746-3508-423f-bad1-5b2f0871ae7b (jobs in lexicogr. order):\n",
      "\n",
      "    [Thu Oct 15 15:46:19 2020]\n",
      "    rule map_back_reads:\n",
      "        input: analysis/spades/scaffolds.fasta, seq/reads-R1.fastq, seq/reads-R2.fastq, analysis/spades/scaffolds.fasta.1.bt2, analysis/spades/scaffolds.fasta.2.bt2, analysis/spades/scaffolds.fasta.3.bt2, analysis/spades/scaffolds.fasta.4.bt2, analysis/spades/scaffolds.fasta.rev.1.bt2, analysis/spades/scaffolds.fasta.rev.2.bt2\n",
      "        output: analysis/unsorted.bam (pipe)\n",
      "        jobid: 4\n",
      "        threads: 3\n",
      "\n",
      "\n",
      "    [Thu Oct 15 15:46:19 2020]\n",
      "    rule map_sort:\n",
      "        input: analysis/unsorted.bam\n",
      "        output: analysis/sorted.bam\n",
      "        jobid: 2\n",
      "\n",
      "[Thu Oct 15 15:47:09 2020]\n",
      "Finished job 4.\n",
      "[Thu Oct 15 15:47:09 2020]\n",
      "Finished job 2.\n",
      "6 of 9 steps (67%) done\n",
      "\n",
      "[Thu Oct 15 15:47:09 2020]\n",
      "rule index_bam:\n",
      "    input: analysis/sorted.bam\n",
      "    output: analysis/sorted.bam.bai\n",
      "    jobid: 3\n",
      "\n",
      "[Thu Oct 15 15:47:09 2020]\n",
      "Finished job 3.\n",
      "7 of 9 steps (78%) done\n",
      "\n",
      "[Thu Oct 15 15:47:09 2020]\n",
      "rule bam_stats:\n",
      "    input: analysis/sorted.bam, analysis/sorted.bam.bai\n",
      "    output: analysis/sorted.bam.idxstats\n",
      "    jobid: 1\n",
      "\n",
      "[Thu Oct 15 15:47:09 2020]\n",
      "Finished job 1.\n",
      "8 of 9 steps (89%) done\n",
      "\n",
      "[Thu Oct 15 15:47:09 2020]\n",
      "rule all:\n",
      "    input: analysis/sorted.bam.idxstats\n",
      "    jobid: 0\n",
      "\n",
      "[Thu Oct 15 15:47:10 2020]\n",
      "Finished job 0.\n",
      "9 of 9 steps (100%) done\n",
      "Complete log: /Users/standage/Projects/snakemod/sandbox/WD1/.snakemake/log/2020-10-15T153601.822053.snakemake.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snakemake.snakemake(\n",
    "    \"Snakefile\", configfiles=[\"config.json\"], cores=4, targets=[\"all\"],\n",
    "    workdir=\"sandbox/WD1/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Includes\n",
    "\n",
    "Snakemake's `include` statement allows a user to import the contents of another Snakefile.\n",
    "This other file can implement an entire workflow itself, or represent only a fragment of a workflow.\n",
    "Snakemake operates as if the user had copied and pasted the contents of the `include`d file into the main Snakefile (except that the default rule is not affected).\n",
    "As a result, the main workflow and all included workflows share a single scope, and thus use a single shared configuration.\n",
    "\n",
    "- pros\n",
    "    - simplest to implement\n",
    "    - included Snakefiles don't have to be free-standing workflows\n",
    "- cons\n",
    "    - shared scope encourages tight coupling of loosely related steps\n",
    "    - shared config not easy to implement\n",
    "    \n",
    "### Exercise 1\n",
    "\n",
    "For our first exercise, let's do the following.\n",
    "\n",
    "1. Make a copy of `Snakefile` that contains only the preprocessing and assembly steps. We could call it `asmbl-inc.smk`.\n",
    "2. Make a copy of `Snakefile` that contains only the mapping and postprocessing steps. We could call it `map-inc.smk`.\n",
    "3. Make a very simple workflow that `include`s those two `.smk` files and implements a single rule. We could call it `Includes.smk`.\n",
    "\n",
    "```python\n",
    "include: \"asmbl-inc.smk\"\n",
    "include: \"map-inc.smk\"\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        \"analysis/sorted.bam.idxstats\",\n",
    "    run:\n",
    "        print(\"Yay, all done!\")\n",
    "```\n",
    "\n",
    "Then we can run the workflow with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake.snakemake(\n",
    "    \"Includes.smk\", configfiles=[\"config.json\"], cores=4, targets=[\"all\"],\n",
    "    workdir=\"sandbox/WD2/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Subworkflows\n",
    "\n",
    "Snakemake subworkflows allow one to invoke a distinct and self-contained workflow from another workflow.\n",
    "Unlike includes, a subworkflow has a scope that is distinct from the Snakefile that calls it.\n",
    "Subworkflows have their own distict configurations and working directories.\n",
    "\n",
    "A subworkflow is imported and named using the `subworkflow` statement.\n",
    "\n",
    "```python\n",
    "subworkflow foobar:\n",
    "    snakefile: \"foobar.smk\"\n",
    "    configfile: \"foobar.json\"\n",
    "```\n",
    "\n",
    "If a rule in the main Snakefile depends on a file built by a subworkflow, simply wrap the filename in the subworkflow's name.\n",
    "\n",
    "```python\n",
    "rule calc_summary:\n",
    "    input: foobar(\"results.csv\")\n",
    "    output: \"summary.dat\"\n",
    "    shell: \"./summary.sh {input} > {output}\"\n",
    "```\n",
    "\n",
    "A Snakemake file can import any number of subworkflows.\n",
    "However, if is not possible (as far as I can tell) to make one subworkflow depend on the output of another subworkflow.\n",
    "This is A Shame, since that is one of the most compelling use cases for subworkflows.\n",
    "\n",
    "- pros\n",
    "    - requires Snakemake files to be well-structured in a way that `include`s don't\n",
    "    - distinct and isolated workflows are arguably better in a design sense\n",
    "- cons\n",
    "    - config file / workdir handling is a bit quirky\n",
    "    - difficult to chain together\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "1. Make a copy of `Snakefile` that contains only the preprocessing and assembly steps. We could call it `asmbl-sub.smk`.\n",
    "2. Make another copy of `Snakefile`. We could call it `Subworkflow.smk`.\n",
    "    - Remove all of the preprocessing and assembly steps.\n",
    "    - Import the subworkflow and name it something incredible clever like `assembly`.\n",
    "    - For the rules in `Subworkflow.smk` that depend on files created by the `asmbl-sub.smk` subworkflow, make sure to wrap those filenames with the subworkflow name (see example below).\n",
    "\n",
    "```python\n",
    "subworkflow assembly:\n",
    "    snakefile: \"asmbl-sub.smk\"\n",
    "    configfile: \"asmbl.json\"\n",
    "\n",
    "rule index_reference:\n",
    "    input:\n",
    "        asmbl=assembly(\"analysis/spades/scaffolds.fasta\")\n",
    "    # ...and so on...\n",
    "```\n",
    "\n",
    "Then we can run the workflow with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake.snakemake(\n",
    "    \"Subworkflow.smk\", configfiles=[\"config.json\"], cores=4, targets=[\"all\"],\n",
    "    workdir=\"sandbox/WD3/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Wrappers\n",
    "\n",
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
