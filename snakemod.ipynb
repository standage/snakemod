{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snakemake\n",
    "from subprocess import run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity in Snakemake workflows\n",
    "\n",
    "The Snakemake workflow engine has proven itself useful in creating, maintaining, and extending a variety of our bioinformatics analysis workflows.\n",
    "We use Snakemake to create both single-purpose workflows as well as workflows that we expect to use and re-use frequently over an extended period of time.\n",
    "Accordingly, we will be faced with two recurrent questions.\n",
    "\n",
    "1. Which parts of which workflows can be re-used?\n",
    "2. What is the best strategy for modularity and re-use?\n",
    "\n",
    "We don't yet have a confident answer for those questions in the context of NBFAC workflows.\n",
    "The purpose of this workshop is to describe three complementary strategies to creating workflows or workflow components that can be maintained in a single place and re-used in numerous contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synopsis\n",
    "\n",
    "1. **Includes** are typically a small set of re-usable rules that get embedded into a larger workflow and share its configuration. This is the simplest way to re-use workflow components.\n",
    "2. **Subworkflows** allow one to invoke a distinct and self-contained workflow from another workflow. Each workflow can has its own configuration, working directory, and so on.\n",
    "3. **Wrappers** are used at the rule level, and provide the smallest building block for constructing workflows from re-usable parts. Snakemake maintains a public repository of wrappers for common tools, but Snakemake can also make use of private or local wrapper definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example workflow\n",
    "\n",
    "![Workflow](workflow.png)\n",
    "\n",
    "The demo workflow for this workshop is a fairly simple and linear workflow. In summary, it:\n",
    "\n",
    "- Downsamples Illumina reads to a user-specified number of read pairs using seqtk\n",
    "- Assembles the downsampled reads using SPAdes\n",
    "- Maps the original reads back to the assembled contigs using Bowtie2\n",
    "- Converts the alignment output in SAM format to BAM format, and sorts the aligned reads by position using SAMtools\n",
    "- Calculates some summary statistics from the read mappings using SAMtools\n",
    "\n",
    "The workflow consists of 9 rules: 3 related to proprocessing and assembly, 5 related to read mapping, and the 1 \"default\" rule to rule them all.\n",
    "With subworkflows and includes, we can explore how to re-use groups of related rules.\n",
    "With wrappers, we can explore how to replace rules running commonly used software with standardized invocation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Before we get started, let's download some test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting SRA file to Fastq format\n",
      "Read 129850 spots for SRR5944233.1.sra\n",
      "Written 129850 spots for SRR5944233.1.sra\n"
     ]
    }
   ],
   "source": [
    "!./getdata.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a reference for our exercises, let's run the example Snakemake workflow using the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tall\n",
      "\t1\tbam_stats\n",
      "\t1\tcopyseq\n",
      "\t1\tdownsample\n",
      "\t1\tindex_assembly\n",
      "\t1\tindex_bam\n",
      "\t1\tmap_back_reads\n",
      "\t1\tmap_sort\n",
      "\t1\tspades\n",
      "\t9\n",
      "\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "rule copyseq:\n",
      "    input: /home/jovyan/sample057-R1.fastq, /home/jovyan/sample057-R2.fastq\n",
      "    output: seq/reads-R1.fastq, seq/reads-R2.fastq\n",
      "    jobid: 6\n",
      "\n",
      "\n",
      "        cp /home/jovyan/sample057-R1.fastq seq/reads-R1.fastq\n",
      "        cp /home/jovyan/sample057-R2.fastq seq/reads-R2.fastq\n",
      "        \n",
      "\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "rule downsample:\n",
      "    input: seq/reads-R1.fastq, seq/reads-R2.fastq\n",
      "    output: seq/reads-subset-R1.fastq, seq/reads-subset-R2.fastq\n",
      "    jobid: 8\n",
      "\n",
      "\n",
      "        seqtk sample -s137720190 seq/reads-R1.fastq 50000 > seq/reads-subset-R1.fastq\n",
      "        seqtk sample -s137720190 seq/reads-R2.fastq 50000 > seq/reads-subset-R2.fastq\n",
      "        \n",
      "\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "rule spades:\n",
      "    input: seq/reads-subset-R1.fastq, seq/reads-subset-R2.fastq\n",
      "    output: analysis/spades/scaffolds.fasta\n",
      "    jobid: 5\n",
      "    threads: 4\n",
      "\n",
      "spades.py -1 seq/reads-subset-R1.fastq -2 seq/reads-subset-R2.fastq -t 4 -o analysis/spades/ --careful\n",
      "\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "rule index_assembly:\n",
      "    input: analysis/spades/scaffolds.fasta\n",
      "    output: analysis/spades/scaffolds.fasta.1.bt2, analysis/spades/scaffolds.fasta.2.bt2, analysis/spades/scaffolds.fasta.3.bt2, analysis/spades/scaffolds.fasta.4.bt2, analysis/spades/scaffolds.fasta.rev.1.bt2, analysis/spades/scaffolds.fasta.rev.2.bt2\n",
      "    jobid: 7\n",
      "\n",
      "bowtie2-build analysis/spades/scaffolds.fasta analysis/spades/scaffolds.fasta\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "\n",
      "group job f75dba4d-cfb7-40c5-9790-a5abd8b621f9 (jobs in lexicogr. order):\n",
      "\n",
      "    [Fri Oct 16 00:33:29 2020]\n",
      "    rule map_back_reads:\n",
      "        input: analysis/spades/scaffolds.fasta, seq/reads-R1.fastq, seq/reads-R2.fastq, analysis/spades/scaffolds.fasta.1.bt2, analysis/spades/scaffolds.fasta.2.bt2, analysis/spades/scaffolds.fasta.3.bt2, analysis/spades/scaffolds.fasta.4.bt2, analysis/spades/scaffolds.fasta.rev.1.bt2, analysis/spades/scaffolds.fasta.rev.2.bt2\n",
      "        output: analysis/unsorted.bam (pipe)\n",
      "        jobid: 4\n",
      "        threads: 3\n",
      "\n",
      "    \n",
      "        bowtie2 -p 3 -x analysis/spades/scaffolds.fasta -1 seq/reads-R1.fastq -2 seq/reads-R2.fastq             | samtools view -q 10 -Sbh > analysis/unsorted.bam\n",
      "        \n",
      "\n",
      "    [Fri Oct 16 00:33:29 2020]\n",
      "    rule map_sort:\n",
      "        input: analysis/unsorted.bam\n",
      "        output: analysis/sorted.bam\n",
      "        jobid: 2\n",
      "\n",
      "    samtools sort -o analysis/sorted.bam analysis/unsorted.bam\n",
      "\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "rule index_bam:\n",
      "    input: analysis/sorted.bam\n",
      "    output: analysis/sorted.bam.bai\n",
      "    jobid: 3\n",
      "\n",
      "samtools index analysis/sorted.bam\n",
      "\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "rule bam_stats:\n",
      "    input: analysis/sorted.bam, analysis/sorted.bam.bai\n",
      "    output: analysis/sorted.bam.idxstats\n",
      "    jobid: 1\n",
      "\n",
      "samtools idxstats analysis/sorted.bam > analysis/sorted.bam.idxstats\n",
      "\n",
      "[Fri Oct 16 00:33:29 2020]\n",
      "rule all:\n",
      "    input: analysis/sorted.bam.idxstats\n",
      "    jobid: 0\n",
      "\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tall\n",
      "\t1\tbam_stats\n",
      "\t1\tcopyseq\n",
      "\t1\tdownsample\n",
      "\t1\tindex_assembly\n",
      "\t1\tindex_bam\n",
      "\t1\tmap_back_reads\n",
      "\t1\tmap_sort\n",
      "\t1\tspades\n",
      "\t9\n",
      "This was a dry-run (flag -n). The order of jobs does not reflect the order of execution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snakemake.snakemake(\n",
    "    \"Snakefile\", configfiles=[\"config.json\"], cores=4, targets=[\"all\"],\n",
    "    printshellcmds=True, workdir=\"sandbox/WD1/\", dryrun=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Includes\n",
    "\n",
    "Snakemake's `include` statement allows a user to import the contents of another Snakefile.\n",
    "This other file can implement an entire workflow itself, or represent only a fragment of a workflow.\n",
    "Snakemake operates as if the user had copied and pasted the contents of the `include`d file into the main Snakefile (except that the default rule is not affected).\n",
    "As a result, the main workflow and all included workflows share a single scope, and thus use a single shared configuration.\n",
    "\n",
    "- pros\n",
    "    - simplest to implement\n",
    "    - included Snakefiles don't have to be free-standing workflows\n",
    "- cons\n",
    "    - shared scope encourages tight coupling of loosely related steps\n",
    "    - shared config not easy to implement\n",
    "    \n",
    "### Exercise 1\n",
    "\n",
    "For our first exercise, let's do the following.\n",
    "\n",
    "1. Make a copy of `Snakefile` that contains only the preprocessing and assembly steps. We could call it `asmbl-inc.smk`.\n",
    "2. Make a copy of `Snakefile` that contains only the mapping and postprocessing steps. We could call it `map-inc.smk`.\n",
    "3. Make a very simple workflow that `include`s those two `.smk` files and implements a single rule. We could call it `Includes.smk`.\n",
    "\n",
    "```python\n",
    "include: \"asmbl-inc.smk\"\n",
    "include: \"map-inc.smk\"\n",
    "\n",
    "rule all:\n",
    "    input:\n",
    "        \"analysis/sorted.bam.idxstats\",\n",
    "    run:\n",
    "        print(\"Yay, all done!\")\n",
    "```\n",
    "\n",
    "Then we test run the workflow with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake.snakemake(\n",
    "    \"Includes.smk\", configfiles=[\"config.json\"], cores=4, targets=[\"all\"],\n",
    "    printshellcmds=True, workdir=\"sandbox/WD2/\", dryrun=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Subworkflows\n",
    "\n",
    "Snakemake subworkflows allow one to invoke a distinct and self-contained workflow from another workflow.\n",
    "Unlike includes, a subworkflow has a scope that is distinct from the Snakefile that calls it.\n",
    "Subworkflows have their own distict configurations and working directories.\n",
    "\n",
    "A subworkflow is imported and named using the `subworkflow` statement.\n",
    "\n",
    "----------\n",
    "```python\n",
    "subworkflow foobar:\n",
    "    snakefile: \"foobar.smk\"\n",
    "    configfile: \"foobar.json\"\n",
    "```\n",
    "----------\n",
    "\n",
    "If a rule in the main Snakefile depends on a file built by a subworkflow, simply wrap the filename in the subworkflow's name.\n",
    "\n",
    "----------\n",
    "```python\n",
    "rule calc_summary:\n",
    "    input: foobar(\"results.csv\")\n",
    "    output: \"summary.dat\"\n",
    "    shell: \"./summary.sh {input} > {output}\"\n",
    "```\n",
    "----------\n",
    "\n",
    "A Snakemake file can import any number of subworkflows.\n",
    "However, if is not possible (as far as I can tell) to make one subworkflow depend on the output of another subworkflow.\n",
    "This is A Shame, since that (in my opinion) is one of the most compelling use cases for subworkflows.\n",
    "\n",
    "- pros\n",
    "    - requires Snakemake files to be well-structured in a way that `include`s don't\n",
    "    - distinct and isolated workflows are arguably better in a design sense\n",
    "- cons\n",
    "    - config file / workdir handling is a bit quirky\n",
    "    - difficult to chain together\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "1. Make a copy of `Snakefile` that contains only the preprocessing and assembly steps. We could call it `asmbl-sub.smk`.\n",
    "2. Make another copy of `Snakefile`. We could call it `Subworkflow.smk`.\n",
    "    - Remove all of the preprocessing and assembly steps.\n",
    "    - Import the subworkflow and name it something incredible clever like `assembly`.\n",
    "    - For the rules in `Subworkflow.smk` that depend on files created by the `asmbl-sub.smk` subworkflow, make sure to wrap those filenames with the subworkflow name (see example below).\n",
    "\n",
    "----------\n",
    "```python\n",
    "subworkflow assembly:\n",
    "    snakefile: \"asmbl-sub.smk\"\n",
    "    configfile: \"asmbl.json\"\n",
    "\n",
    "rule index_reference:\n",
    "    input:\n",
    "        asmbl=assembly(\"analysis/spades/scaffolds.fasta\")\n",
    "    # ...and so on...\n",
    "```\n",
    "----------\n",
    "\n",
    "Then we can test the workflow with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake.snakemake(\n",
    "    \"Subworkflow.smk\", configfiles=[\"config.json\"], cores=4, targets=[\"all\"],\n",
    "    printshellcmds=True, workdir=\"sandbox/WD3/\", dryrun=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3: Wrappers\n",
    "\n",
    "[Snakemake wrappers](https://snakemake-wrappers.readthedocs.io/en/stable/) provide a means to rapidly integrate popular bioinformatics tools into a workflow.\n",
    "Unlike includes and subworkflows, wrappers operate at the level of individual rules, rather than groups of related rules.\n",
    "And wrappers are not mutually exclusive with includes and subworkflows: it is possible to use wrappers in the main Snakemake file, any `include`d Snakemake files, and/or any subworkflows.\n",
    "\n",
    "The Snakemake wrappers website lists the tools for which wrappers are available.\n",
    "Each tool provides an example of how to invoke it, including labels for input and output files.\n",
    "Rules using wrappers will look very similar to regular rules, except the `shell` or `run` block is replaced with a `wrapper` block that specifies the wrapper to be used.\n",
    "\n",
    "----------\n",
    "```python\n",
    "rule downsample:\n",
    "    input:\n",
    "        f1=\"seq/reads-R1.fastq\",\n",
    "        f2=\"seq/reads-R2.fastq\",\n",
    "    output:\n",
    "        f1=\"seq/reads-subset-R1.fastq.gz\",\n",
    "        f2=\"seq/reads-subset-R2.fastq.gz\",\n",
    "    params:\n",
    "        n=config[\"sample_numreads\"],\n",
    "        seed=config[\"sample_seed\"],\n",
    "    wrapper:\n",
    "        \"0.64.0/bio/seqtk/subsample/pe\"\n",
    "```\n",
    "----------\n",
    "\n",
    "- pros\n",
    "    - clean, concise rules\n",
    "    - taking advantage of network effects of community conventions\n",
    "- cons\n",
    "    - may restrict flexibility\n",
    "    - limited benefit in some cases\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "1. Make a copy of `Snakefile`. We could call it `Wrappers.smk`.\n",
    "2. Refer to the Snakemake wrappers website to see which tools in the workflow already have wrappers. Use the examples listed on the website to update those rules and use the wrapper.\n",
    "\n",
    "Then we can test the workflow with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakemake.snakemake(\n",
    "    \"Wrappers.smk\", configfiles=[\"config.json\"], cores=4, targets=[\"all\"],\n",
    "    printshellcmds=True, workdir=\"sandbox/WD4/\", dryrun=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
